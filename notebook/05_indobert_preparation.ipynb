{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9d3c9c",
   "metadata": {},
   "source": [
    "# 05 â€” Persiapan Data untuk Fine-Tuning IndoBERT\n",
    "\n",
    "Notebook ini berisi tahapan persiapan data untuk pemodelan berbasis **IndoBERT**.\n",
    "Berbeda dengan TF-IDF yang menggunakan teks hasil preprocessing lengkap, IndoBERT\n",
    "membutuhkan teks yang lebih alami agar konteks bahasa tidak hilang.\n",
    "\n",
    "Pada tahap ini dilakukan:\n",
    "\n",
    "1. Membaca dataset hasil preprocessing penuh.\n",
    "2. Memilih kolom teks **raw (`comment`)** untuk IndoBERT.\n",
    "3. Menyiapkan label numerik.\n",
    "4. Membagi dataset menjadi train, validation, dan test.\n",
    "5. Membuat dataset HuggingFace.\n",
    "6. Melakukan tokenisasi menggunakan tokenizer IndoBERT.\n",
    "7. Mengonversi dataset ke format PyTorch untuk proses training.\n",
    "\n",
    "Notebook ini merupakan tahap sebelum fine-tuning IndoBERT pada notebook selanjutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00ac0a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1dc49",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0779e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428d132",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190d44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataset_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b646d",
   "metadata": {},
   "source": [
    "## Gunakan RAW COMMENT sebagai input IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5aa390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8611328b-dd44-458d-b386-dc33cf43e9e9",
       "rows": [
        [
         "0",
         "Yg benci ya apa aja salah.. \nYg seneng ya makin seneng... \nYg nyinyir y dpt bahan.. \nYg dukung y anggap kesuksesan.. \nTerserah rakyat nusantara aja...",
         "neutral",
         "1"
        ],
        [
         "1",
         "Bandung akan miliki kereta pajajaran dgn beaya LBH murah  trs whoosh BS balik modal gak tu ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…ðŸ˜…",
         "neutral",
         "1"
        ],
        [
         "2",
         "SUDAH JELAS GENG SOLO YANG HARUS BERTANGGUNG JAWAB tangkap semua",
         "negative",
         "0"
        ],
        [
         "3",
         "Jokowi, Luhut, kroni2  yg harus bertanggungjawab dan Bayar hutang WOSH semua, \nMereka dalang dari kerugian Negara Selama ini,\n\nInilah akibat OKNUM Presiden dan Pejabat Negara, aparat hukum ygMengikuti Nafsu Setan, terlalu Rakus, licik dan Serakah",
         "negative",
         "0"
        ],
        [
         "4",
         "Yg ditangkap gorengan yg makan duduk manis",
         "neutral",
         "1"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yg benci ya apa aja salah.. \\nYg seneng ya mak...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bandung akan miliki kereta pajajaran dgn beaya...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUDAH JELAS GENG SOLO YANG HARUS BERTANGGUNG J...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jokowi, Luhut, kroni2  yg harus bertanggungjaw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yg ditangkap gorengan yg makan duduk manis</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  label\n",
       "0  Yg benci ya apa aja salah.. \\nYg seneng ya mak...   neutral      1\n",
       "1  Bandung akan miliki kereta pajajaran dgn beaya...   neutral      1\n",
       "2  SUDAH JELAS GENG SOLO YANG HARUS BERTANGGUNG J...  negative      0\n",
       "3  Jokowi, Luhut, kroni2  yg harus bertanggungjaw...  negative      0\n",
       "4         Yg ditangkap gorengan yg makan duduk manis   neutral      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"comment\"].astype(str)\n",
    "df[[\"text\", \"sentiment\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db0ad4",
   "metadata": {},
   "source": [
    "## Drop NA dan duplikat jika masih ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4049315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 987\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Total data:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f724f5",
   "metadata": {},
   "source": [
    "## Trainâ€“Validationâ€“Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14774a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 99, 99)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train (80%) dan temp (20%)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# split temp jadi validation (10%) dan test (10%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df[\"label\"]\n",
    ")\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb198c",
   "metadata": {},
   "source": [
    "## Siapkan HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2762c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
    "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fbebb",
   "metadata": {},
   "source": [
    "## Load Tokenizer IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e5211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hafizh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hafizh\\.cache\\huggingface\\hub\\models--indobenchmark--indobert-base-p2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"indobenchmark/indobert-base-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2da7f",
   "metadata": {},
   "source": [
    "## Fungsi Tokenisasi Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b64f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b25871",
   "metadata": {},
   "source": [
    "## Tokenisasi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698c69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 789/789 [00:00<00:00, 10564.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 8807.26 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 9046.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = datasets.map(tokenize_batch, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1ccf2",
   "metadata": {},
   "source": [
    "## Bersihkan Kolom & Ganti Nama Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c90c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = tokenized_ds.remove_columns([\"text\"])\n",
    "\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds.set_format(type=\"torch\")\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ddc7b2",
   "metadata": {},
   "source": [
    "## Save HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a86ec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 789/789 [00:00<00:00, 104655.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 19070.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 14115.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.save_to_disk(\"../data/indobert_tokenized/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
