{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3388b674",
   "metadata": {},
   "source": [
    "# 01 ‚Äì Labeling Sentimen dengan Gemini (LLM-Assisted Labeling)\n",
    "\n",
    "Notebook ini digunakan untuk melakukan **labeling** sentimen komentar YouTube\n",
    "terkait **dugaan korupsi proyek Kereta Cepat Whoosh (Jakarta-Bandung)**, menggunakan **Gemini API**.\n",
    "\n",
    "## Alur Proses\n",
    "\n",
    "1. **Load Dataset**  \n",
    "   Membaca `data/raw_dataset_whoosh.csv` (hasil scraping dari 10 video YouTube, ¬±1000 komentar)\n",
    "\n",
    "2. **Konfigurasi Gemini API**  \n",
    "   Menggunakan API key yang disimpan di file `.env`\n",
    "\n",
    "3. **Skema Label Sentimen**  \n",
    "   - **Positif**: Komentar yang membela, mendukung, atau menilai proyek Whoosh/pemerintah secara baik terkait isu korupsi\n",
    "   - **Negatif**: Komentar yang mengkritik, menuduh korupsi, atau menilai proyek/pemerintah secara buruk\n",
    "   - **Netral**: Komentar yang informatif, bertanya, bercanda, atau tidak menunjukkan opini jelas\n",
    "\n",
    "4. **Batch Processing**  \n",
    "   Melabeli komentar secara bertahap (batch) dengan delay antar batch untuk menghindari rate limit API\n",
    "\n",
    "5. **Output Final**  \n",
    "   Hasil akhir disimpan ke `data/labeled_dataset_whoosh.csv`\n",
    "\n",
    "6. **Review & Koreksi Manual** (Opsional)  \n",
    "   Dataset ini dapat direview dan dikoreksi manual untuk meningkatkan akurasi\n",
    "\n",
    "## Catatan Penting\n",
    "\n",
    "‚ö†Ô∏è **Label dari Gemini = pseudo-label (label awal)**  \n",
    "- Bukan kebenaran mutlak, masih perlu validasi\n",
    "- Akurasi tergantung pada kualitas prompt dan kemampuan model\n",
    "\n",
    "‚úÖ **Kualitas akhir dikontrol dengan:**  \n",
    "- Prompt engineering yang baik\n",
    "- Sampling & review hasil labeling\n",
    "- Koreksi manual untuk data penting (opsional)\n",
    "\n",
    "## Parameter yang Digunakan\n",
    "\n",
    "- **Batch Size**: 25 komentar per batch\n",
    "- **Delay**: 60 detik antar batch\n",
    "- **Model**: `gemini-2.0-flash`\n",
    "- **Total Dataset**: ~1000 komentar dari 10 video YouTube\n",
    "\n",
    "## File Output\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ raw_dataset_whoosh.csv          # Input (hasil scraping)\n",
    "‚îî‚îÄ‚îÄ labeled_dataset_whoosh.csv      # Output final (dengan kolom 'sentiment')\n",
    "```\n",
    "\n",
    "## Konteks Dataset\n",
    "\n",
    "Dataset ini berisi komentar publik dari YouTube terkait isu dugaan korupsi pada proyek Kereta Cepat Whoosh. \n",
    "Analisis sentimen dilakukan untuk memahami opini publik terhadap isu tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6c3a7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d105185",
   "metadata": {},
   "source": [
    "## Install Dependensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai python-dotenv tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e372244",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b8ea",
   "metadata": {},
   "source": [
    "## Konfigurasi API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY tidak ditemukan di file .env\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT LABELING - WHOOSH DATASET\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198bbc1",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25      # Jumlah komentar per batch\n",
    "DELAY = 60           # Delay antar batch (detik)\n",
    "RETRY_DELAY = 120    # Delay saat kena rate limit (detik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5952c",
   "metadata": {},
   "source": [
    "## Path File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "RAW_PATH = os.path.join(DATA_DIR, \"raw_dataset_whoosh.csv\")\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, \"labeled_dataset_whoosh.csv\")\n",
    "CHECKPOINT_PATH = os.path.join(DATA_DIR, \"checkpoint_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaf954",
   "metadata": {},
   "source": [
    "## Fungsi Klasifikasi Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53150f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment_batch(comments):\n",
    "    \"\"\"\n",
    "    Klasifikasi sentimen untuk batch komentar sekaligus.\n",
    "    \"\"\"\n",
    "    numbered_comments = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(comments)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Anda adalah analis sentimen publik khusus untuk isu *dugaan korupsi proyek Kereta Cepat Whoosh* \n",
    "(Jakarta‚ÄîBandung). Tugas Anda adalah menilai apakah setiap komentar menunjukkan sentimen \n",
    "positif, netral, atau negatif *terhadap isu dugaan korupsi tersebut*.\n",
    "\n",
    "Fokus utama:\n",
    "- Nilai sentimen berdasarkan sikap komentar terhadap *dugaan korupsi Whoosh*, \n",
    "  bukan sekadar terhadap layanan Whoosh sebagai kereta cepat.\n",
    "\n",
    "Pedoman penilaian:\n",
    "1. **Positive**\n",
    "   - Mendukung, membela, atau tidak percaya bahwa ada korupsi.\n",
    "   - Menganggap isu korupsi tidak benar, dilebih-lebihkan, atau ada pihak yang menyebarkan hoaks.\n",
    "   - Menilai proyek berjalan baik dan tidak berkaitan dengan korupsi.\n",
    "\n",
    "2. **Negative**\n",
    "   - Menyatakan bahwa proyek Whoosh memang korup, merugikan negara, penuh penyimpangan.\n",
    "   - Menyalahkan pemerintah, pejabat, atau pihak tertentu terkait dugaan korupsi proyek tersebut.\n",
    "   - Mengkritik biaya, pembengkakan anggaran, atau tudingan penyalahgunaan dana.\n",
    "\n",
    "3. **Neutral**\n",
    "   - Tidak menunjukkan opini jelas.\n",
    "   - Hanya bertanya, bercanda, atau menceritakan informasi umum.\n",
    "   - Komentar tidak relevan dengan isu korupsi.\n",
    "\n",
    "Jumlah komentar: {len(comments)}.\n",
    "\n",
    "Berikut daftar komentar yang harus Anda klasifikasikan sesuai urutan:\n",
    "\n",
    "{numbered_comments}\n",
    "\n",
    "Format jawaban:\n",
    "- Jawab HANYA dengan Python list berisi label untuk setiap komentar.\n",
    "- Gunakan **huruf kecil semua**: \"positive\", \"negative\", \"neutral\".\n",
    "- Contoh format:\n",
    "  [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "Jumlah elemen dalam list HARUS tepat {len(comments)}.\n",
    "Jangan tambahkan kata lain, penjelasan, alasan, nomor, atau teks di luar list.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    raw_output = response.text.strip()\n",
    "    \n",
    "    # Bersihkan format markdown jika ada\n",
    "    raw_output = re.sub(r\"^```(?:python)?\", \"\", raw_output)\n",
    "    raw_output = re.sub(r\"```$\", \"\", raw_output).strip()\n",
    "    \n",
    "    # Parsing hasil\n",
    "    try:\n",
    "        labels = re.findall(r'\"(.*?)\"', raw_output)\n",
    "        if not labels:  # Jika tidak ada tanda kutip, coba pisah berdasarkan koma\n",
    "            labels = [w.strip(\" []'\\\"\\n\") for w in raw_output.split(\",\") if w.strip()]\n",
    "        \n",
    "        # Validasi jumlah label\n",
    "        if len(labels) != len(comments):\n",
    "            print(f\"    [WARNING] Jumlah label ({len(labels)}) ‚â† jumlah komentar ({len(comments)})\")\n",
    "            if len(labels) < len(comments):\n",
    "                labels += [\"Netral\"] * (len(comments) - len(labels))\n",
    "            else:\n",
    "                labels = labels[:len(comments)]\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"    [ERROR] Format output tidak valid:\")\n",
    "        print(f\"    {raw_output[:200]}...\")\n",
    "        return [\"Netral\"] * len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e27fa",
   "metadata": {},
   "source": [
    "## Fungsi Proses Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiment_labeling(df, batch_size=50, delay=60):\n",
    "    \"\"\"\n",
    "    Proses labeling sentimen dengan batch processing dan checkpoint.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    \n",
    "    # Cek checkpoint\n",
    "    start_idx = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        df_checkpoint = pd.read_csv(CHECKPOINT_PATH)\n",
    "        start_idx = len(df_checkpoint)\n",
    "        all_labels = df_checkpoint['sentiment'].tolist()\n",
    "        print(f\"\\n[INFO] Melanjutkan dari checkpoint: {start_idx} komentar sudah dilabel\")\n",
    "    \n",
    "    total_batches = (len(df) - start_idx + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\n[INFO] Total komentar: {len(df)}\")\n",
    "    print(f\"[INFO] Batch size: {batch_size}\")\n",
    "    print(f\"[INFO] Total batches: {total_batches}\")\n",
    "    print(f\"[INFO] Starting from index: {start_idx}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Loop setiap batch\n",
    "    for i in range(start_idx, len(df), batch_size):\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        # Ambil batch komentar\n",
    "        batch_comments = df['comment'].iloc[i:i+batch_size].astype(str).tolist()\n",
    "        \n",
    "        print(f\"\\n[Batch {batch_num}/{total_batches}] Processing {len(batch_comments)} komentar...\")\n",
    "        \n",
    "        # Retry mechanism untuk rate limit\n",
    "        while True:\n",
    "            try:\n",
    "                labels = classify_sentiment_batch(batch_comments)\n",
    "                print(f\"    ‚úì Berhasil! Labels: {dict(pd.Series(labels).value_counts())}\")\n",
    "                break  # keluar dari loop jika berhasil\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"    [RATE LIMIT] Terkena limit API. Menunggu {RETRY_DELAY}s...\")\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(f\"    [ERROR] {e}\")\n",
    "                    labels = [\"Netral\"] * len(batch_comments)\n",
    "                    break\n",
    "        \n",
    "        # Simpan hasil batch\n",
    "        all_labels.extend(labels)\n",
    "        \n",
    "        # Simpan checkpoint\n",
    "        df_checkpoint = pd.DataFrame({\n",
    "            \"video_id\": df[\"video_id\"].iloc[:len(all_labels)],\n",
    "            \"video_title\": df[\"video_title\"].iloc[:len(all_labels)],\n",
    "            \"comment_id\": df[\"comment_id\"].iloc[:len(all_labels)],\n",
    "            \"author\": df[\"author\"].iloc[:len(all_labels)],\n",
    "            \"comment\": df[\"comment\"].iloc[:len(all_labels)],\n",
    "            \"likes\": df[\"likes\"].iloc[:len(all_labels)],\n",
    "            \"published_at\": df[\"published_at\"].iloc[:len(all_labels)],\n",
    "            \"sentiment\": all_labels\n",
    "        })\n",
    "        df_checkpoint.to_csv(CHECKPOINT_PATH, index=False, encoding='utf-8')\n",
    "        print(f\"    üíæ Checkpoint saved ({len(all_labels)}/{len(df)} komentar)\")\n",
    "        \n",
    "        # Delay antar batch\n",
    "        if i + batch_size < len(df):\n",
    "            print(f\"    ‚è≥ Menunggu {delay}s sebelum batch berikutnya...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Hasil akhir\n",
    "    df_result = df.copy()\n",
    "    df_result['sentiment'] = all_labels\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005cd25",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b410fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[1] Loading dataset...\")\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    print(f\"    Total komentar: {len(df)}\")\n",
    "    \n",
    "    # Filter komentar kosong jika perlu\n",
    "    df = df[df['comment'].notna()].reset_index(drop=True)\n",
    "    print(f\"    Setelah filter: {len(df)} komentar\")\n",
    "    \n",
    "    # Preview 5 komentar pertama\n",
    "    print(\"\\n[2] Preview data:\")\n",
    "    print(df[['comment']].head())\n",
    "    \n",
    "    # Proses labeling\n",
    "    print(\"\\n[3] Mulai proses labeling...\")\n",
    "    df_result = process_sentiment_labeling(\n",
    "        df, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        delay=DELAY\n",
    "    )\n",
    "    \n",
    "    # Simpan hasil akhir\n",
    "    print(f\"\\n[4] Menyimpan hasil ke {OUTPUT_PATH}...\")\n",
    "    df_result.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Statistik hasil\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HASIL LABELING FINAL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal komentar: {len(df_result)}\")\n",
    "    print(\"\\nDistribusi Sentimen:\")\n",
    "    sentiment_counts = df_result['sentiment'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(\"\\nPersentase:\")\n",
    "    print(df_result['sentiment'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Hapus checkpoint setelah selesai\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "        print(\"\\n‚úì Checkpoint dihapus\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ PROSES SELESAI!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nFile tersimpan di: {OUTPUT_PATH}\")\n",
    "    \n",
    "    # Preview hasil\n",
    "    print(\"\\nüìä Preview hasil labeling:\")\n",
    "    print(df_result[['comment', 'sentiment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fed0e",
   "metadata": {},
   "source": [
    "## Dataset Hasil Labeling dengan Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2955e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling = pd.read_csv(\"../data/labeled_dataset_whoosh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
