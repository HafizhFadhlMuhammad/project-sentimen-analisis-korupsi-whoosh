{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3388b674",
   "metadata": {},
   "source": [
    "# 01 â€“ Labeling Sentimen dengan Gemini (LLM-Assisted Labeling)\n",
    "\n",
    "Notebook ini digunakan untuk melakukan **labeling** sentimen komentar YouTube\n",
    "terkait **dugaan korupsi proyek Kereta Cepat Whoosh (Jakarta-Bandung)**, menggunakan **Gemini API**.\n",
    "\n",
    "## Alur Proses\n",
    "\n",
    "1. **Load Dataset**  \n",
    "   Membaca `data/raw_dataset_whoosh.csv` (hasil scraping dari 10 video YouTube, Â±1000 komentar)\n",
    "\n",
    "2. **Konfigurasi Gemini API**  \n",
    "   Menggunakan API key yang disimpan di file `.env`\n",
    "\n",
    "3. **Skema Label Sentimen**  \n",
    "   - **Positif**: Komentar yang membela, mendukung, atau menilai proyek Whoosh/pemerintah secara baik terkait isu korupsi\n",
    "   - **Negatif**: Komentar yang mengkritik, menuduh korupsi, atau menilai proyek/pemerintah secara buruk\n",
    "   - **Netral**: Komentar yang informatif, bertanya, bercanda, atau tidak menunjukkan opini jelas\n",
    "\n",
    "4. **Batch Processing**  \n",
    "   Melabeli komentar secara bertahap (batch) dengan delay antar batch untuk menghindari rate limit API\n",
    "\n",
    "5. **Output Final**  \n",
    "   Hasil akhir disimpan ke `data/labeled_dataset_whoosh.csv`\n",
    "\n",
    "6. **Review & Koreksi Manual** (Opsional)  \n",
    "   Dataset ini dapat direview dan dikoreksi manual untuk meningkatkan akurasi\n",
    "\n",
    "## Catatan Penting\n",
    "\n",
    "âš ï¸ **Label dari Gemini = pseudo-label (label awal)**  \n",
    "- Bukan kebenaran mutlak, masih perlu validasi\n",
    "- Akurasi tergantung pada kualitas prompt dan kemampuan model\n",
    "\n",
    "âœ… **Kualitas akhir dikontrol dengan:**  \n",
    "- Prompt engineering yang baik\n",
    "- Sampling & review hasil labeling\n",
    "- Koreksi manual untuk data penting (opsional)\n",
    "\n",
    "## Parameter yang Digunakan\n",
    "\n",
    "- **Batch Size**: 25 komentar per batch\n",
    "- **Delay**: 60 detik antar batch\n",
    "- **Model**: `gemini-2.0-flash`\n",
    "- **Total Dataset**: ~1000 komentar dari 10 video YouTube\n",
    "\n",
    "## File Output\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ raw_dataset_whoosh.csv          # Input (hasil scraping)\n",
    "â””â”€â”€ labeled_dataset_whoosh.csv      # Output final (dengan kolom 'sentiment')\n",
    "```\n",
    "\n",
    "## Konteks Dataset\n",
    "\n",
    "Dataset ini berisi komentar publik dari YouTube terkait isu dugaan korupsi pada proyek Kereta Cepat Whoosh. \n",
    "Analisis sentimen dilakukan untuk memahami opini publik terhadap isu tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6c3a7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d105185",
   "metadata": {},
   "source": [
    "## Install Dependensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a10488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.40.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hafizh\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hafizh\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed protobuf-5.29.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "langchain-google-genai 2.1.8 requires google-ai-generativelanguage<0.7.0,>=0.6.18, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 5.29.5 which is incompatible.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-dotenv tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e372244",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8de857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b8ea",
   "metadata": {},
   "source": [
    "## Konfigurasi API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c864b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTIMENT LABELING - WHOOSH DATASET\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY tidak ditemukan di file .env\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT LABELING - WHOOSH DATASET\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198bbc1",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25      # Jumlah komentar per batch\n",
    "DELAY = 60           # Delay antar batch (detik)\n",
    "RETRY_DELAY = 120    # Delay saat kena rate limit (detik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5952c",
   "metadata": {},
   "source": [
    "## Path File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae56b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "RAW_PATH = os.path.join(DATA_DIR, \"raw_dataset_whoosh.csv\")\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, \"labeled_dataset_whoosh.csv\")\n",
    "CHECKPOINT_PATH = os.path.join(DATA_DIR, \"checkpoint_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaf954",
   "metadata": {},
   "source": [
    "## Fungsi Klasifikasi Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53150f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment_batch(comments):\n",
    "    \"\"\"\n",
    "    Klasifikasi sentimen untuk batch komentar sekaligus.\n",
    "    \"\"\"\n",
    "    numbered_comments = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(comments)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Anda adalah analis sentimen publik khusus untuk isu *dugaan korupsi proyek Kereta Cepat Whoosh* \n",
    "(Jakartaâ€”Bandung). Tugas Anda adalah menilai apakah setiap komentar menunjukkan sentimen \n",
    "positif, netral, atau negatif *terhadap isu dugaan korupsi tersebut*.\n",
    "\n",
    "Fokus utama:\n",
    "- Nilai sentimen berdasarkan sikap komentar terhadap *dugaan korupsi Whoosh*, \n",
    "  bukan sekadar terhadap layanan Whoosh sebagai kereta cepat.\n",
    "\n",
    "Pedoman penilaian:\n",
    "1. **Positive**\n",
    "   - Mendukung, membela, atau tidak percaya bahwa ada korupsi.\n",
    "   - Menganggap isu korupsi tidak benar, dilebih-lebihkan, atau ada pihak yang menyebarkan hoaks.\n",
    "   - Menilai proyek berjalan baik dan tidak berkaitan dengan korupsi.\n",
    "\n",
    "2. **Negative**\n",
    "   - Menyatakan bahwa proyek Whoosh memang korup, merugikan negara, penuh penyimpangan.\n",
    "   - Menyalahkan pemerintah, pejabat, atau pihak tertentu terkait dugaan korupsi proyek tersebut.\n",
    "   - Mengkritik biaya, pembengkakan anggaran, atau tudingan penyalahgunaan dana.\n",
    "\n",
    "3. **Neutral**\n",
    "   - Tidak menunjukkan opini jelas.\n",
    "   - Hanya bertanya, bercanda, atau menceritakan informasi umum.\n",
    "   - Komentar tidak relevan dengan isu korupsi.\n",
    "\n",
    "Jumlah komentar: {len(comments)}.\n",
    "\n",
    "Berikut daftar komentar yang harus Anda klasifikasikan sesuai urutan:\n",
    "\n",
    "{numbered_comments}\n",
    "\n",
    "Format jawaban:\n",
    "- Jawab HANYA dengan Python list berisi label untuk setiap komentar.\n",
    "- Gunakan **huruf kecil semua**: \"positive\", \"negative\", \"neutral\".\n",
    "- Contoh format:\n",
    "  [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "Jumlah elemen dalam list HARUS tepat {len(comments)}.\n",
    "Jangan tambahkan kata lain, penjelasan, alasan, nomor, atau teks di luar list.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    raw_output = response.text.strip()\n",
    "    \n",
    "    # Bersihkan format markdown jika ada\n",
    "    raw_output = re.sub(r\"^```(?:python)?\", \"\", raw_output)\n",
    "    raw_output = re.sub(r\"```$\", \"\", raw_output).strip()\n",
    "    \n",
    "    # Parsing hasil\n",
    "    try:\n",
    "        labels = re.findall(r'\"(.*?)\"', raw_output)\n",
    "        if not labels:  # Jika tidak ada tanda kutip, coba pisah berdasarkan koma\n",
    "            labels = [w.strip(\" []'\\\"\\n\") for w in raw_output.split(\",\") if w.strip()]\n",
    "        \n",
    "        # Validasi jumlah label\n",
    "        if len(labels) != len(comments):\n",
    "            print(f\"    [WARNING] Jumlah label ({len(labels)}) â‰  jumlah komentar ({len(comments)})\")\n",
    "            if len(labels) < len(comments):\n",
    "                labels += [\"Netral\"] * (len(comments) - len(labels))\n",
    "            else:\n",
    "                labels = labels[:len(comments)]\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"    [ERROR] Format output tidak valid:\")\n",
    "        print(f\"    {raw_output[:200]}...\")\n",
    "        return [\"Netral\"] * len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e27fa",
   "metadata": {},
   "source": [
    "## Fungsi Proses Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiment_labeling(df, batch_size=50, delay=60):\n",
    "    \"\"\"\n",
    "    Proses labeling sentimen dengan batch processing dan checkpoint.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    \n",
    "    # Cek checkpoint\n",
    "    start_idx = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        df_checkpoint = pd.read_csv(CHECKPOINT_PATH)\n",
    "        start_idx = len(df_checkpoint)\n",
    "        all_labels = df_checkpoint['sentiment'].tolist()\n",
    "        print(f\"\\n[INFO] Melanjutkan dari checkpoint: {start_idx} komentar sudah dilabel\")\n",
    "    \n",
    "    total_batches = (len(df) - start_idx + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\n[INFO] Total komentar: {len(df)}\")\n",
    "    print(f\"[INFO] Batch size: {batch_size}\")\n",
    "    print(f\"[INFO] Total batches: {total_batches}\")\n",
    "    print(f\"[INFO] Starting from index: {start_idx}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Loop setiap batch\n",
    "    for i in range(start_idx, len(df), batch_size):\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        # Ambil batch komentar\n",
    "        batch_comments = df['comment'].iloc[i:i+batch_size].astype(str).tolist()\n",
    "        \n",
    "        print(f\"\\n[Batch {batch_num}/{total_batches}] Processing {len(batch_comments)} komentar...\")\n",
    "        \n",
    "        # Retry mechanism untuk rate limit\n",
    "        while True:\n",
    "            try:\n",
    "                labels = classify_sentiment_batch(batch_comments)\n",
    "                print(f\"    âœ“ Berhasil! Labels: {dict(pd.Series(labels).value_counts())}\")\n",
    "                break  # keluar dari loop jika berhasil\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"    [RATE LIMIT] Terkena limit API. Menunggu {RETRY_DELAY}s...\")\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(f\"    [ERROR] {e}\")\n",
    "                    labels = [\"Netral\"] * len(batch_comments)\n",
    "                    break\n",
    "        \n",
    "        # Simpan hasil batch\n",
    "        all_labels.extend(labels)\n",
    "        \n",
    "        # Simpan checkpoint\n",
    "        df_checkpoint = pd.DataFrame({\n",
    "            \"video_id\": df[\"video_id\"].iloc[:len(all_labels)],\n",
    "            \"video_title\": df[\"video_title\"].iloc[:len(all_labels)],\n",
    "            \"comment_id\": df[\"comment_id\"].iloc[:len(all_labels)],\n",
    "            \"author\": df[\"author\"].iloc[:len(all_labels)],\n",
    "            \"comment\": df[\"comment\"].iloc[:len(all_labels)],\n",
    "            \"likes\": df[\"likes\"].iloc[:len(all_labels)],\n",
    "            \"published_at\": df[\"published_at\"].iloc[:len(all_labels)],\n",
    "            \"sentiment\": all_labels\n",
    "        })\n",
    "        df_checkpoint.to_csv(CHECKPOINT_PATH, index=False, encoding='utf-8')\n",
    "        print(f\"    ðŸ’¾ Checkpoint saved ({len(all_labels)}/{len(df)} komentar)\")\n",
    "        \n",
    "        # Delay antar batch\n",
    "        if i + batch_size < len(df):\n",
    "            print(f\"    â³ Menunggu {delay}s sebelum batch berikutnya...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Hasil akhir\n",
    "    df_result = df.copy()\n",
    "    df_result['sentiment'] = all_labels\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005cd25",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b410fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading dataset...\n",
      "    Total komentar: 1968\n",
      "    Setelah filter: 1968 komentar\n",
      "\n",
      "[2] Preview data:\n",
      "                                             comment\n",
      "0  Pake tanya lagi yg di tangkap ya pelakunya kal...\n",
      "1  apakah ini termasuk fomo? atau ambisi tak berd...\n",
      "2                Dalang semua adalah luhut panjaitan\n",
      "3  Yang masih membanggakan tukang kayu sini gue g...\n",
      "4  YANG HARUS DITANGKAP ADALAH DALANG YANG MENDAT...\n",
      "\n",
      "[3] Mulai proses labeling...\n",
      "\n",
      "[INFO] Total komentar: 1968\n",
      "[INFO] Batch size: 25\n",
      "[INFO] Total batches: 79\n",
      "[INFO] Starting from index: 0\n",
      "============================================================\n",
      "\n",
      "[Batch 1/79] Processing 25 komentar...\n",
      "    [RATE LIMIT] Terkena limit API. Menunggu 120s...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[1] Loading dataset...\")\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    print(f\"    Total komentar: {len(df)}\")\n",
    "    \n",
    "    # Filter komentar kosong jika perlu\n",
    "    df = df[df['comment'].notna()].reset_index(drop=True)\n",
    "    print(f\"    Setelah filter: {len(df)} komentar\")\n",
    "    \n",
    "    # Preview 5 komentar pertama\n",
    "    print(\"\\n[2] Preview data:\")\n",
    "    print(df[['comment']].head())\n",
    "    \n",
    "    # Proses labeling\n",
    "    print(\"\\n[3] Mulai proses labeling...\")\n",
    "    df_result = process_sentiment_labeling(\n",
    "        df, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        delay=DELAY\n",
    "    )\n",
    "    \n",
    "    # Simpan hasil akhir\n",
    "    print(f\"\\n[4] Menyimpan hasil ke {OUTPUT_PATH}...\")\n",
    "    df_result.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Statistik hasil\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HASIL LABELING FINAL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal komentar: {len(df_result)}\")\n",
    "    print(\"\\nDistribusi Sentimen:\")\n",
    "    sentiment_counts = df_result['sentiment'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(\"\\nPersentase:\")\n",
    "    print(df_result['sentiment'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Hapus checkpoint setelah selesai\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "        print(\"\\nâœ“ Checkpoint dihapus\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… PROSES SELESAI!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nFile tersimpan di: {OUTPUT_PATH}\")\n",
    "    \n",
    "    # Preview hasil\n",
    "    print(\"\\nðŸ“Š Preview hasil labeling:\")\n",
    "    print(df_result[['comment', 'sentiment']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fed0e",
   "metadata": {},
   "source": [
    "## Dataset Hasil Labeling dengan Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2955e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling = pd.read_csv(\"../data/labeled_dataset_whoosh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
