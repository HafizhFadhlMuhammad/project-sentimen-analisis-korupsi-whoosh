{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3388b674",
   "metadata": {},
   "source": [
    "# 01 â€“ Labeling Sentimen dengan Gemini (LLM-Assisted Labeling)\n",
    "\n",
    "Notebook ini digunakan untuk melakukan **pseudo-labeling** sentimen komentar YouTube\n",
    "terkait isu dugaan korupsi proyek Kereta Cepat Whoosh, menggunakan **Gemini API**.\n",
    "\n",
    "Alur di notebook ini:\n",
    "\n",
    "1. Load dataset `data/raw_dataset_whoosh.csv` (hasil scraping 10 video, Â±1000 komentar)\n",
    "2. Konfigurasi Gemini (menggunakan API key di `.env`)\n",
    "3. Menentukan skema label: `positive`, `neutral`, `negative`\n",
    "4. Melabeli komentar secara bertahap (batch) dengan jeda (delay) agar tidak kena rate limit\n",
    "5. Menyimpan hasil ke `data/pseudo_labels_batch_*.csv` dan `data/pseudo_labels_all.csv`\n",
    "6. Dataset ini akan direview & dikoreksi manual â†’ menjadi `final_label.csv`\n",
    "\n",
    "**Catatan:**\n",
    "- Label dari Gemini = **label awal (pseudo-label)**, bukan kebenaran mutlak.\n",
    "- Kualitas akhir tetap dikontrol dengan **koreksi manual**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d105185",
   "metadata": {},
   "source": [
    "## Install Dependensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a10488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.40.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hafizh\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hafizh\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hafizh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-dotenv tqdm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e372244",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8de857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b8ea",
   "metadata": {},
   "source": [
    "## Konfigurasi API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c864b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SENTIMENT LABELING - WHOOSH DATASET\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if GEMINI_API_KEY is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY tidak ditemukan di file .env\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT LABELING - WHOOSH DATASET\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198bbc1",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50      # Jumlah komentar per batch\n",
    "DELAY = 60           # Delay antar batch (detik)\n",
    "RETRY_DELAY = 120    # Delay saat kena rate limit (detik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5952c",
   "metadata": {},
   "source": [
    "## Path File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae56b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "RAW_PATH = os.path.join(DATA_DIR, \"raw_dataset_whoosh.csv\")\n",
    "OUTPUT_PATH = os.path.join(DATA_DIR, \"labeled_dataset_whoosh.csv\")\n",
    "CHECKPOINT_PATH = os.path.join(DATA_DIR, \"checkpoint_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaf954",
   "metadata": {},
   "source": [
    "## Fungsi Klasifikasi Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53150f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment_batch(comments):\n",
    "    \"\"\"\n",
    "    Klasifikasi sentimen untuk batch komentar sekaligus.\n",
    "    \"\"\"\n",
    "    numbered_comments = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(comments)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Anda adalah analis sentimen publik khusus untuk isu *dugaan korupsi proyek Kereta Cepat Whoosh* \n",
    "(Jakartaâ€”Bandung). Tugas Anda adalah menilai apakah setiap komentar menunjukkan sentimen \n",
    "positif, netral, atau negatif *terhadap isu dugaan korupsi tersebut*.\n",
    "\n",
    "Fokus utama:\n",
    "- Nilai sentimen berdasarkan sikap komentar terhadap *dugaan korupsi Whoosh*, \n",
    "  bukan sekadar terhadap layanan Whoosh sebagai kereta cepat.\n",
    "\n",
    "Pedoman penilaian:\n",
    "1. **Positive**\n",
    "   - Mendukung, membela, atau tidak percaya bahwa ada korupsi.\n",
    "   - Menganggap isu korupsi tidak benar, dilebih-lebihkan, atau ada pihak yang menyebarkan hoaks.\n",
    "   - Menilai proyek berjalan baik dan tidak berkaitan dengan korupsi.\n",
    "\n",
    "2. **Negative**\n",
    "   - Menyatakan bahwa proyek Whoosh memang korup, merugikan negara, penuh penyimpangan.\n",
    "   - Menyalahkan pemerintah, pejabat, atau pihak tertentu terkait dugaan korupsi proyek tersebut.\n",
    "   - Mengkritik biaya, pembengkakan anggaran, atau tudingan penyalahgunaan dana.\n",
    "\n",
    "3. **Neutral**\n",
    "   - Tidak menunjukkan opini jelas.\n",
    "   - Hanya bertanya, bercanda, atau menceritakan informasi umum.\n",
    "   - Komentar tidak relevan dengan isu korupsi.\n",
    "\n",
    "Jumlah komentar: {len(comments)}.\n",
    "\n",
    "Berikut daftar komentar yang harus Anda klasifikasikan sesuai urutan:\n",
    "\n",
    "{numbered_comments}\n",
    "\n",
    "Format jawaban:\n",
    "- Jawab HANYA dengan Python list berisi label untuk setiap komentar.\n",
    "- Gunakan **huruf kecil semua**: \"positive\", \"negative\", \"neutral\".\n",
    "- Contoh format:\n",
    "  [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "Jumlah elemen dalam list HARUS tepat {len(comments)}.\n",
    "Jangan tambahkan kata lain, penjelasan, alasan, nomor, atau teks di luar list.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    raw_output = response.text.strip()\n",
    "    \n",
    "    # Bersihkan format markdown jika ada\n",
    "    raw_output = re.sub(r\"^```(?:python)?\", \"\", raw_output)\n",
    "    raw_output = re.sub(r\"```$\", \"\", raw_output).strip()\n",
    "    \n",
    "    # Parsing hasil\n",
    "    try:\n",
    "        labels = re.findall(r'\"(.*?)\"', raw_output)\n",
    "        if not labels:  # Jika tidak ada tanda kutip, coba pisah berdasarkan koma\n",
    "            labels = [w.strip(\" []'\\\"\\n\") for w in raw_output.split(\",\") if w.strip()]\n",
    "        \n",
    "        # Validasi jumlah label\n",
    "        if len(labels) != len(comments):\n",
    "            print(f\"    [WARNING] Jumlah label ({len(labels)}) â‰  jumlah komentar ({len(comments)})\")\n",
    "            if len(labels) < len(comments):\n",
    "                labels += [\"Netral\"] * (len(comments) - len(labels))\n",
    "            else:\n",
    "                labels = labels[:len(comments)]\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"    [ERROR] Format output tidak valid:\")\n",
    "        print(f\"    {raw_output[:200]}...\")\n",
    "        return [\"Netral\"] * len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e27fa",
   "metadata": {},
   "source": [
    "## Fungsi Proses Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522721c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiment_labeling(df, batch_size=50, delay=60):\n",
    "    \"\"\"\n",
    "    Proses labeling sentimen dengan batch processing dan checkpoint.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    \n",
    "    # Cek checkpoint\n",
    "    start_idx = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        df_checkpoint = pd.read_csv(CHECKPOINT_PATH)\n",
    "        start_idx = len(df_checkpoint)\n",
    "        all_labels = df_checkpoint['sentiment'].tolist()\n",
    "        print(f\"\\n[INFO] Melanjutkan dari checkpoint: {start_idx} komentar sudah dilabel\")\n",
    "    \n",
    "    total_batches = (len(df) - start_idx + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\n[INFO] Total komentar: {len(df)}\")\n",
    "    print(f\"[INFO] Batch size: {batch_size}\")\n",
    "    print(f\"[INFO] Total batches: {total_batches}\")\n",
    "    print(f\"[INFO] Starting from index: {start_idx}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Loop setiap batch\n",
    "    for i in range(start_idx, len(df), batch_size):\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        # Ambil batch komentar\n",
    "        batch_comments = df['comment'].iloc[i:i+batch_size].astype(str).tolist()\n",
    "        \n",
    "        print(f\"\\n[Batch {batch_num}/{total_batches}] Processing {len(batch_comments)} komentar...\")\n",
    "        \n",
    "        # Retry mechanism untuk rate limit\n",
    "        while True:\n",
    "            try:\n",
    "                labels = classify_sentiment_batch(batch_comments)\n",
    "                print(f\"    âœ“ Berhasil! Labels: {dict(pd.Series(labels).value_counts())}\")\n",
    "                break  # keluar dari loop jika berhasil\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                if \"RESOURCE_EXHAUSTED\" in error_str or \"429\" in error_str:\n",
    "                    print(f\"    [RATE LIMIT] Terkena limit API. Menunggu {RETRY_DELAY}s...\")\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(f\"    [ERROR] {e}\")\n",
    "                    labels = [\"Netral\"] * len(batch_comments)\n",
    "                    break\n",
    "        \n",
    "        # Simpan hasil batch\n",
    "        all_labels.extend(labels)\n",
    "        \n",
    "        # Simpan checkpoint\n",
    "        df_checkpoint = pd.DataFrame({\n",
    "            \"video_id\": df[\"video_id\"].iloc[:len(all_labels)],\n",
    "            \"video_title\": df[\"video_title\"].iloc[:len(all_labels)],\n",
    "            \"comment_id\": df[\"comment_id\"].iloc[:len(all_labels)],\n",
    "            \"author\": df[\"author\"].iloc[:len(all_labels)],\n",
    "            \"comment\": df[\"comment\"].iloc[:len(all_labels)],\n",
    "            \"likes\": df[\"likes\"].iloc[:len(all_labels)],\n",
    "            \"published_at\": df[\"published_at\"].iloc[:len(all_labels)],\n",
    "            \"sentiment\": all_labels\n",
    "        })\n",
    "        df_checkpoint.to_csv(CHECKPOINT_PATH, index=False, encoding='utf-8')\n",
    "        print(f\"    ðŸ’¾ Checkpoint saved ({len(all_labels)}/{len(df)} komentar)\")\n",
    "        \n",
    "        # Delay antar batch\n",
    "        if i + batch_size < len(df):\n",
    "            print(f\"    â³ Menunggu {delay}s sebelum batch berikutnya...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Hasil akhir\n",
    "    df_result = df.copy()\n",
    "    df_result['sentiment'] = all_labels\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005cd25",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b410fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading dataset...\n",
      "    Total komentar: 1000\n",
      "    Setelah filter: 1000 komentar\n",
      "\n",
      "[2] Preview data:\n",
      "                                             comment\n",
      "0  Yg benci ya apa aja salah.. \\nYg seneng ya mak...\n",
      "1  Bandung akan miliki kereta pajajaran dgn beaya...\n",
      "2  SUDAH JELAS GENG SOLO YANG HARUS BERTANGGUNG J...\n",
      "3  Jokowi, Luhut, kroni2  yg harus bertanggungjaw...\n",
      "4         Yg ditangkap gorengan yg makan duduk manis\n",
      "\n",
      "[3] Mulai proses labeling...\n",
      "\n",
      "[INFO] Total komentar: 1000\n",
      "[INFO] Batch size: 50\n",
      "[INFO] Total batches: 20\n",
      "[INFO] Starting from index: 0\n",
      "============================================================\n",
      "\n",
      "[Batch 1/20] Processing 50 komentar...\n",
      "    [RATE LIMIT] Terkena limit API. Menunggu 120s...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[1] Loading dataset...\")\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    print(f\"    Total komentar: {len(df)}\")\n",
    "    \n",
    "    # Filter komentar kosong jika perlu\n",
    "    df = df[df['comment'].notna()].reset_index(drop=True)\n",
    "    print(f\"    Setelah filter: {len(df)} komentar\")\n",
    "    \n",
    "    # Preview 5 komentar pertama\n",
    "    print(\"\\n[2] Preview data:\")\n",
    "    print(df[['comment']].head())\n",
    "    \n",
    "    # Proses labeling\n",
    "    print(\"\\n[3] Mulai proses labeling...\")\n",
    "    df_result = process_sentiment_labeling(\n",
    "        df, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        delay=DELAY\n",
    "    )\n",
    "    \n",
    "    # Simpan hasil akhir\n",
    "    print(f\"\\n[4] Menyimpan hasil ke {OUTPUT_PATH}...\")\n",
    "    df_result.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Statistik hasil\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HASIL LABELING FINAL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal komentar: {len(df_result)}\")\n",
    "    print(\"\\nDistribusi Sentimen:\")\n",
    "    sentiment_counts = df_result['sentiment'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(\"\\nPersentase:\")\n",
    "    print(df_result['sentiment'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Hapus checkpoint setelah selesai\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "        print(\"\\nâœ“ Checkpoint dihapus\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… PROSES SELESAI!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nFile tersimpan di: {OUTPUT_PATH}\")\n",
    "    \n",
    "    # Preview hasil\n",
    "    print(\"\\nðŸ“Š Preview hasil labeling:\")\n",
    "    print(df_result[['comment', 'sentiment']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
